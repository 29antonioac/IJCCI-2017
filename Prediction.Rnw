\documentclass[a4paper,twoside]{article}

\usepackage{epsfig}
\usepackage{subfigure}
\usepackage{calc}
\usepackage{amssymb}
\usepackage{amstext}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{multicol}
\usepackage{pslatex}
\usepackage{apalike}
\usepackage{float}
\usepackage{tabularx}
\usepackage{SCITEPRESS}     % Please add other packages that you may need BEFORE the SCITEPRESS.sty package.

\subfigtopskip=0pt
\subfigcapskip=0pt
\subfigbottomskip=0pt

\begin{document}

\title{Early prediction of the winner in \emph{StarCraft} matches }

\author{\authorname{Antonio \'Alvarez-Caballero\sup{1}, J.J. Merelo\sup{1} and Third Author Name\sup{2}}
\affiliation{\sup{1}Department of Computer Architecture and Computer Technology, University of Granada, Spain}
\email{analca3@correo.ugr.es, jmerelo@ugr.es}
}

\keywords{Prediction, Classification, Strategy, Planification}

\abstract{}

\onecolumn \maketitle \normalsize \vfill

<<setup, include=FALSE, cache=FALSE>>=
library(knitr)
library(ggplot2)
library(reshape2)

knitr::opts_chunk$set(echo = F, fig.pos = "H")
#knit_hooks$set(document = function(x) {sub('\\usepackage[]{color}', '\\usepackage[pdftex,dvipsnames]{xcolor}', x, fixed = TRUE)})

readSparkData <- function(folder) {
  dfs <- lapply(list.files(folder, pattern = "*.csv", full.names = TRUE), read.csv)
  df <- do.call(rbind, dfs)
  df
}

# opts_chunk$set(cache=TRUE, autodep=TRUE)
# opts_chunk$set(fig.width=4, fig.height=3,fig.pos = '!hbt')
#opts_chunk$set(fig.pos = 'H', cache=T, autodep=T)
#options(scipen=999)
@

\section{\uppercase{Introduction}}
\label{sec:introduction}

% Supervised learning is...

\noindent Supervised learning is a very useful approach to solve problems using
related data. In the field of AI applied to videogames research, its use is
conditional on the possibility of getting data from them.

% Introducing the game and its relation with supervised learning

Real Time Strategy or \emph{RTS} games are a very suitable kind of videogames
to use supervised learning with, because usually they have a very large set of
features which could be used to analyze the game deeply. In particular,
\emph{StarCraft} is a RTS game from the 90s. In this game both
players begin with a simple building and some workers, and the objective is
to create an army that can defeat the opponent's one. This is ussualy achieved
gathering resources from the map and building some important structures to get
the best units for your army. However, there are infinite strategies to follow,
and all of them are valid ones. Whatever strategy it is used, this process
generates a lot of data, which can be used to get hidden information from the
match.

% BWAPI: Accesing to data

Usually companies don't allow access matches' data to users. \emph{StarCraft}
is not an exception, but community have created an \emph{API} to access data
and manipulate the game itself: \emph{BWAPI}. With this tool users can create
artificial agent which play the game making competitions. Another common use
of this \emph{API} is gather data. The conclusions that can be extracted
from data could be very important, because could offer extra information
of matches in real time.

% Our proposal

In this paper we prove that with a well designed set of \emph{StarCraft}
features, the winner of a match can be predicted accurately, even in an early
stage of the match.
The framework we have chosen for this
work is the Apache ecosystem for data analytics, \emph{Spark} with the machine
learning library \emph{MLlib}. They provide tools able to deal with a big
amount of data, so we think it is a good decision due to the data dimension:
the analysis was done in an usual personal computer.

% What did we do?

In this work a complete Knowledge Discovery in Databases (\emph{KDD}) process
is done. The data were collected from \cite{DBLP:conf/flairs/RobertsonW14},
a set of six relational databases which contains a very big amount of data
from more than 4500 \emph{StarCraft} replays. A preprocessing with \emph{SQL}
was made to organise the data and extract our set of important features. At
last, the modelling was made using \emph{Spark} and \emph{MLlib} as we said,
allowing us to extract useful information as the winner in an early stage of
the matches and a ranking of useful features.

% What did we get?

The main conclusion we get is that we can predict the winner without playing
the whole match. With 10 minutes approximately,
it is enough to get predictions with an accuracy ratio of 90\%. Keeping in mind
that the average duration of a match is 48 minutes approximately, the time
reduction is considerable. It could be useful combined with metaheuristics to
optimize agents for this videogame faster. Furthermore, an agent with the
possibility of predicting accurately the winner could adapt its strategy to
change the outcome of the match.

This could not be achieved without a good set of features. Training a classifier
is easy, but it doesn't help if data has no
quality. This set of data and features could be used in other works based
on \emph{StarCraft} data to try to improve their results.


\section{\uppercase{State of the art}}
\label{sec:state}

\noindent In the \emph{StarCraft} research a lot of approaches has been presented.
The most used approach is developing probabilistic graphical models to predict
the winner of a match. Some examples are in~\cite{DBLP:conf/cig/SynnaeveB11a}
and~\cite{DBLP:conf/aiide/StanescuHEGB13}, where important events in the match
are used to predict the outcome: when a very important building appears,
an important event succession for a race, the birth of the best unit of a race,
etc.

Another approach based in supervised learning is presented
in~\cite{DBLP:conf/cosecivi/Sanchez-Ruiz15}, but the environment is homogeneus
and controlled. It is possible that it doesn't show the diversity in
\emph{StarCraft} matches. A better dataset is presented
in~\cite{DBLP:conf/flairs/RobertsonW14}, which is very heterogeneus,
complete and granulated.

Another works look for plans and strategies based on predictions of the outcome
of matches, as we can see in~\cite{adaptativeStrategyPrediction} and
in~\cite{makingAndActing}.

Another approach is developing strategies using \emph{Genetic Programming},
creating plans automatically which can win. This kind of algorithms are very
time consuming, so whatever saved time would be appreciated. This approach
gives good results, as we can see in~\cite{DBLP:conf/evoW/Fernandez-AresG16}
and~\cite{DBLP:conf/cig/Garcia-SanchezT15}.

\section{\uppercase{Metodology}}
\label{sec:metodology}

In this paper we do a complete \emph{KDD} process using \emph{SQL} and some
Apache tools: \emph{Spark} with its \emph{Scala} interface and \emph{MLlib}.
We did this election because Apache echosystem is suitable for dealing
with very large datasets, offering a framework which produces similar projects
in centralized and distributed environments.

Using its \emph{Scala} interface was chosen because it is the most complete
one for \emph{Spark} and \emph{MLlib}. Furthermore, \emph{Scala} is a modern
functional and object-oriented language which is used widely in some companies
as LinkedIn, Twitter or Siemens. One of this advantages is that \emph{Scala}
compiles to the Java Virtual Machine or \emph{JVM}. As a consecuence,
multiplatform code is developed.

\subsection{Feature selection}
\label{subsec:selection}

% What features we use

The data we use is taken from~\cite{DBLP:conf/flairs/RobertsonW14}, who with
their work offer six relational databases of one versus one matches, with all
the possible combinations of races that the game offers.

In Figure~\ref{fig:databaseSelection} we could see the entity-relationship diagram
of the databases that contain the matches. Understand all features was easy
because~\cite{DBLP:conf/flairs/RobertsonW14} work is totally open, so we could
explore the code associated. Furthermore, a lot of features has the same name
that attributes from the \emph{BWAPI}, although a set of features was calculated
by the researchers like the distance to the base in a moment of the match.

To get a rows and columns dataset, we propose this structure. Each row of the
dataset will be a precise instant of the match, determined by a \emph{Frame}.
Each instant has the information of resources of each player. This approach
is different to another ones presented in~\ref{sec:introduction}. It seems easy
but the organisation of the data was not trivial.

\begin{figure*}[!h]
  \centering
  \includegraphics[scale=0.5]{figure/Robertson14DatabaseDiagramSeleccion.pdf}
  \caption{Selected features in databases. Original diagram by~\cite{DBLP:conf/flairs/RobertsonW14}}
  \label{fig:databaseSelection}
\end{figure*}

We present here the list of selected features, also exposed in
Figure~\ref{fig:databaseSelection}. Some of them are used only to organise the
data, the identifiers of replay, player and region.

\begin{itemize}
  \item replay: This table contais data about each match.
  \begin{itemize}
    \item ReplayID: Match identifier.
    %\item Duration: Duration of each match in frames, 15 frames per second.
  \end{itemize}
  \item playerreplay: This table contains data about a player in a match.
  \begin{itemize}
    \item PlayerReplayID: Player identifier.
    \item ReplayID: Match identifier.
    \item Winner: Winner of the match.
  \end{itemize}
  \item resourcechange: This table contains data associated to changes in player's resources.
  \begin{itemize}
    \item PlayerReplayID: Player identifier.
    \item Frame: Frame when the resource changes.
    \item Minerals: Amount of minerals of a player.
    \item Gas: Amount of gas of a player.
    \item Supply: Carrying capacity of a player.
    \item TotalMinerals: Total amount of minerals of a player, without costs.
    \item TotalGas: Total amount of minerals of a player, without costs.
    \item TotalSupply: Total amount of minerals of a player, without costs.
  \end{itemize}
  \item regionvaluechange: This table contains data associated to changes of a
  player in a map region. Each \emph{value} is the sum of the price of an unit,
  expressed as minerals and gas.
  \begin{itemize}
    \item PlayerReplayID: Player identifier.
    \item RegionID: Region of the map identifier.
    \item Frame: Frame when the value changes.
    \item GroundUnitValue: Value of ground units in this region.
    \item BuildingValue: Value of buildings in this region.
    \item AirUnitValue: Value of air units in this region.
    \item EnemyGroundUnitValue: Value of enemy ground units in this region.
    This value is estimated, the player knows the units they see of the enemy.
    \item EnemyBuildingValue: Value of enemy buildings in this region.
    This value is estimated, the player knows the units they see of the enemy.
    \item EnemyAirUnitValue: Value of enemy air units in this region.
    This value is estimated, the player knows the units they see of the enemy.
    \item ResourceValue: Value of resources in this region. This value is
    estimated, the player knows the units they see of the map.
    If the player doesn't know a region, they estimate this value as the maximum
    available in the region.
  \end{itemize}
\end{itemize}

The features related to a player are presented twice, one for each player.
Furthermore, a \emph{race} feature is used, as the races implied in the match.
As a consecuence, we have 28 features used in the modelling stage of the work.

\subsection{Data preprocessing}
\label{subsec:preprocess}

As we said in~\ref{subsec:selection}, each observation of the data has 28
features related to the resources and units of each player. We have to note
some little tips about the data organisation.

\begin{itemize}
  \item Values of \emph{Frame} are the instants when a whatever player has
  a minerals, gas or supply change. We did this election because in this games
  the basic resources are always changing due to necessity of them to buy
  units or buildings. This implies some missing values in the resources of
  the player who doesn't make a change in that frame, so we had to recover the
  last value of that resource to keep consistency.
  \item Values of \emph{value} of units and buildings doesn't occur in the
  same instant that the resources' changes. For this reason, as we did with
  \emph{Frame}, we had to recover the last \emph{value} of the region
  \emph{values} of each player, too.
  \item \emph{Value} depends of the region of the map. It's important in our
  approach to get a full \emph{value} measure, a value that represents all
  the player units or buildings, so we had to sum the \emph{value} of every
  region of the map.
\end{itemize}


% \subsection{Exploratory Data Analysis}
% \label{subsec:eda}

\subsection{Modelling}
\label{subsec:modelling}

We have chosen for the modelling some algorithms. Some of them are very simple,
because we want to test the complexity of the data.

\begin{itemize}
  \item \emph{Naive Bayes} (NB)
  \item \emph{Logistic Regression} (LR)
  \item \emph{KNN} (KNN)
  \item \emph{Multilayer perceptron} (MLP)
  \item \emph{Random Forest} (RF)
  \item \emph{Gradient Boosting Tree} (GBT)
\end{itemize}

The final results are gotten using this approach 5 times:

\begin{enumerate}
  \item Split data using 70\% to train and 30\% to validation.
  \item Inside the first partition, train a model using 10-fold Cross Validation.
  \item With the final model, validate it in the second partition.
\end{enumerate}

Using this approach, we get 5 accuracy measures for each model, so we can test
the obtained samples using appropiate statistical tests. We use \emph{Friedman
test} to get statistical differences among all the classifiers, and a
pairwise \emph{t-test} to get differences between classifiers.

\section{\uppercase{Results}}
\label{sec:results}

In Figure~\ref{fig:MeasuresGraphics} and table~\ref{tab:measures} is
exposed the accuracy of each model with their parameters. The accuracy is
evaluated in the validation sets.

<<measures>>=
measures.full.path <- "data/measures_full.csv"
measures.full <- read.csv(measures.full.path)
frames <- sort(unique(measures.full$Frames))

max.duration <- max(measures.full$Frames)
avg.duration.frames <- 43968.9184653
frames.to.mins <- 15*60
avg.duration.mins <- avg.duration.frames / frames.to.mins

measures.mean <- aggregate(measures.full[,3:8], by = list(Frames = measures.full$Frames), mean)
measures.median <- aggregate(measures.full[,3:8], by = list(Frames = measures.full$Frames), median)
measures.sd <- aggregate(measures.full[,3:8], by = list(Frames = measures.full$Frames), sd)
measures.norm <- aggregate(measures.full[,3:8], by = list(Frames = measures.full$Frames), function(v) shapiro.test(v)$p.value > 0.05)

mean.melt <- melt(measures.mean, id.vars = "Frames", variable.name = "Classifier", value.name = "Accuracy")
median.melt <- melt(measures.median, id.vars = "Frames", variable.name = "Classifier", value.name = "Accuracy")
sd.melt <- melt(measures.sd, id.vars = "Frames", variable.name = "Classifier", value.name = "SD")

normality <- all(measures.norm[,2:7] == T)
mean_or_median.sd <- merge( if (normality) mean.melt else median.melt,
                            sd.melt,
                            by = c("Frames", "Classifier"))
@

Median with standard deviation is shown because data is not centered around the
mean in all the selected instants.

<<MeasuresData,results='asis'>>=
results <- subset(mean_or_median.sd, Frames == max.duration)

params <- data.frame( Classifier = c("GBT","LR","NB","RF","MLP","KNN"),
                      Params = c("numTrees = 150, maxDepth = 10","maxIter = 150, regParam = 0.3",
                     "Smoothing = 1", "numTrees = 150, maxDepth = 10",
                     "Hidden layers = (10,10)", "K = 3"))
results <- merge(results, params, by = "Classifier")

rownames(results) <- NULL
# print(xtable::xtable(params),
#   floating.environment = "table")
print(xtable::xtable(results[,-2],
  caption = "Trained algorithms with their final parameters",
  table.placement="H",
  label = "tab:measures",
  digits = 6 ),
  floating.environment = "table*")
@

In general, all classifiers but \emph{Naive Bayes} predict pretty well, but
there are two clear winners: \emph{Gradient Boosting Tree} and \emph{KNN}.
This one offers a great accuracy with a very simple approach, but it is not
suitable for doing a lot of predictions in real time, because of its lazy
approach.

<<MeasuresGraphics, fig.cap = "Obtained measures with all models", fig.env = "figure">>=

ggplot(subset(mean_or_median.sd, Frames == max.duration), aes(Classifier, Accuracy, fill = Classifier)) +
  geom_bar(stat = "identity") +
  geom_errorbar(aes(ymin=Accuracy-SD, ymax=Accuracy+SD),width=.3) +
  scale_y_continuous(breaks = seq(0, 1, by = 0.1)) #+
  # theme(axis.text.x = element_text(angle = 45, vjust = 0.8, hjust = 0.8), axis.text.y = element_text(size = 8))
@

<<TimeComparisonAcc, fig.cap = "Classifiers comparative over time", fig.width = 10 >>=
ggplot(subset(mean_or_median.sd, Frames != max.duration), aes(Frames, Accuracy, col = Classifier)) +
  geom_point() +
  geom_line() +
  geom_errorbar(aes(ymin=Accuracy-SD, ymax=Accuracy+SD)) +
  scale_x_continuous(breaks = frames) +
  scale_y_continuous(breaks = seq(0.4,1.0,by=0.05))
@

In figure~\ref{fig:TimeComparisonAcc} is exposed the accuracy differences
between classifiers over time. \emph{KNN} can classify with a precision of $\Sexpr{subset(mean_or_median.sd, Classifier == "KNN" & Frames == 9000)[1,"Accuracy"]} \pm \Sexpr{subset(mean_or_median.sd, Classifier == "KNN" & Frames == 9000)[1,"SD"]}$ only
with 10 minutes of match. The mean of the duration is \Sexpr{avg.duration.frames}
frames, which are equal to \Sexpr{avg.duration.mins} minutes. It implies that
with only a \Sexpr{9000 / avg.duration.frames * 100}\% of the mean duration
of the match, we can predict accurately the winner of a match. It is not
necessary to play the whole match to get the winner with high confidence.

To complete the study, we present a Friedman test to see significative
differences over the classifiers. All frames of the matches are used. We
can see in table~\ref{tab:acc} the accuracy of each classifier.

<<Friedman, results = "asis">>=
d <- as.matrix(subset(measures.full, Frames == max.duration)[,-c(1,2)])
fr <- friedman.test(d)

rownames(d) <- NULL

print(xtable::xtable(d,
  caption = "Accurate of classifiers in each validation set",
  table.placement = "H",
  label = "tab:acc",
  digits = 6),
  floating.environment = "table*")
@

With a p-value of \Sexpr{I(fr$p.value)}, we can confirm that exist statistical
signification. We can see in table~\ref{tab:pvalues} a pairwise test, with
\emph{Bonferroni} adjust method of the p-value.

<<Pairwise, results = "asis">>=
tam <- dim(d)
groups <- rep(1:tam[2], each=tam[1])

a <- pairwise.t.test(d, g = groups, paired = T)
out <- a$p.value
colnames(out) <- colnames(measures.full)[3:7]
rownames(out) <- colnames(measures.full)[4:8]
print(xtable::xtable(out,
  caption = "P-values from a pairwise t-test",
  table.placement="H",
  label = "tab:pvalues",
  digits = 6 ),
  floating.environment = "table*")
@

As we can see, we can confirm that with the usual signification level, 0.05,
there are statistical differences among all classifiers but \emph{Logistic
Regression} and \emph{Multilayer Perceptron}. They are not the best classifiers
so this is not important. The important fact is that there are statistical
differences between \emph{KNN} and \emph{Gradient Boosting Tree}.

\section{\uppercase{Conclusions}}
\label{sec:conclusions}

With this study we can extract some conclusions. The first one is that the set
of features are well formed: the winner can be predicted from the selected
features with high confidence, as we exposed in~\ref{sec:results}.

The second and main conclusion is that predictions with this data are very
accurate in an early stage, in particular using a \emph{KNN}
classifier. It can predict with 90\% of accuracy using 10 minutes of match only.
This is very important because it proofs that it is not necessary to play a
whole match to predict the winner accurately.

As future work there are some research opportunities using this study. With a
\emph{KNN} classifier, a competitive \emph{bot} could be developed with an
important skill: the winner's prediction. It could be used to improve the
adaptability of the agent, giving advantage to its opponents.

Another work could be the improvement of \emph{bot} optimization using the early
winner knowledge. We could use this knowledge to improve the evaluation step of
the algorithms, giving the opportunity to use more exhaustive setups on the
algorithms.





\section*{\uppercase{Acknowledgements}}
\label{sec:acknowledgements}

This work has been supported in part by: de Ministerio espa\~{n}ol de
Econom\'{\i}a y Competitividad under project TIN2014-56494-C4-3-P
(UGR-EPHEMECH) and by CONACYT PEI Project No. 220590. 

\vfill
\bibliographystyle{apalike}
{\small
\bibliography{prediction}}

\vfill
\end{document}
